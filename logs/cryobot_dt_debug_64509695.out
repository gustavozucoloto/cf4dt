HOME_REAL:      /rwthfs/rz/cluster/home/kq336027
MAMBA_ROOT:     /rwthfs/rz/cluster/home/kq336027/.micromamba
Job started on: n23m0130.hpc.itc.rwth-aachen.de
Project dir:    /rwthfs/rz/cluster/home/kq336027/cf4dt
CPUs per task:  16
MAMBA_ROOT:     /rwthfs/rz/cluster/home/kq336027/.micromamba

=== Sanity check: Python + MPI vendor ===
Python OK
MPI vendor: ('MPICH', (4, 3, 2))
MPI library: MPICH Version:      4.3.2
COMM_WORLD size: 1

=== Step 1: Generating artificial data -> data/artificial_Qlc_data.csv ===
Note: Edit velocity/temperature lists and n_jobs in scripts/generate_artificial_data.py
Running 20 simulations serially...
  Progress: 10/20
  Progress: 20/20

Saved dataset with 20 points to: data/artificial_Qlc_data.csv
Qlc_true_kW: min/median/max = 0.3637554679858434 5.0027582811749545 20.798917773919225

============================================================
MODEL: powerlaw
============================================================
=== Step 2: Training GP emulator -> data/gp_powerlaw.joblib ===
[powerlaw] Pre-compiling FEniCS forms to avoid race conditions...
[powerlaw] Building training set with 16 parallel processes...
  [powerlaw] Progress: 50/400
  [powerlaw] Progress: 100/400
  [powerlaw] Progress: 150/400
  [powerlaw] Progress: 200/400
  [powerlaw] Progress: 250/400
  [powerlaw] Progress: 300/400
  [powerlaw] Progress: 400/400
  [powerlaw] Progress: 350/400
Saved GP emulator: data/gp_powerlaw.joblib
Kernel: 10**2 * RBF(length_scale=[1.36, 3.67, 15, 22.4]) + WhiteKernel(noise_level=1e-08)

=== Step 3: Bayesian calibration (emcee) -> data/posterior_powerlaw.npy ===
Saved posterior samples to data/posterior_powerlaw.npy
powerlaw posterior mean: [-14.37431819   0.72077578] std: [0.32447123 0.45913799]

=== Step 4: UQ plots -> outputs/uq_powerlaw* ===

============================================================
MODEL: exponential
============================================================
=== Step 2: Training GP emulator -> data/gp_exponential.joblib ===
[exponential] Pre-compiling FEniCS forms to avoid race conditions...
[exponential] Building training set with 16 parallel processes...
  [exponential] Progress: 50/400
  [exponential] Progress: 100/400
  [exponential] Progress: 150/400
  [exponential] Progress: 200/400
  [exponential] Progress: 250/400
  [exponential] Progress: 300/400
  [exponential] Progress: 400/400
  [exponential] Progress: 350/400
Saved GP emulator: data/gp_exponential.joblib
Kernel: 10**2 * RBF(length_scale=[1.34, 3.28, 15.2, 21.9]) + WhiteKernel(noise_level=1e-08)
Training data: 400 points saved in bundle

=== Step 3: Bayesian calibration (emcee) -> data/posterior_exponential.npy ===
Saved posterior samples to data/posterior_exponential.npy
exponential posterior mean: [-1.43769965e+01  3.01059896e-03] std: [0.3291042  0.00263216]

=== Step 4: UQ plots -> outputs/uq_exponential* ===

All done.
